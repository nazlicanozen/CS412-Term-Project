{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBOlFYpnlr0N"
      },
      "source": [
        "# Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PM5EnXhLk6BL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gzip\n",
        "import json\n",
        "\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgAdz_b8xp8w",
        "outputId": "98d6730e-45e8-40bd-b338-606399944452"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#@title Turkish StopWords\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "turkish_stopwords = stopwords.words('turkish')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC7KXsQZL7Kp"
      },
      "source": [
        "# Like Count Prediction\n",
        "\n",
        "\n",
        "Here, we use the average like_count of the user's previous posts to predict each post's like_count"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import json\n",
        "import random\n",
        "\n",
        "# Path to the gzipped dataset\n",
        "data_path = \"training-dataset.jsonl.gz\"\n",
        "\n",
        "# Initialize dictionaries for train and test data\n",
        "train_data = {\"profiles\": {}, \"posts\": {}}\n",
        "test_data = {\"profiles\": {}, \"posts\": {}}\n",
        "\n",
        "# Load and shuffle the data\n",
        "data_entries = []\n",
        "\n",
        "with gzip.open(data_path, \"rt\") as fh:\n",
        "    for line in fh:\n",
        "        sample = json.loads(line)\n",
        "        data_entries.append(sample)\n",
        "\n",
        "# Shuffle the data to ensure randomness\n",
        "random.shuffle(data_entries)\n",
        "\n",
        "# Define the train-test split ratio\n",
        "train_ratio = 0.8\n",
        "train_size = int(len(data_entries) * train_ratio)\n",
        "\n",
        "# Divide the data into train and test sets\n",
        "train_post_count = 0\n",
        "test_post_count = 0\n",
        "\n",
        "for i, sample in enumerate(data_entries):\n",
        "    profile = sample[\"profile\"]\n",
        "    username = profile[\"username\"]\n",
        "    posts = sample[\"posts\"]\n",
        "\n",
        "    if i < train_size:\n",
        "        train_data[\"profiles\"][username] = profile\n",
        "        train_data[\"posts\"][username] = posts\n",
        "        train_post_count += len(posts)\n",
        "    else:\n",
        "        test_data[\"profiles\"][username] = profile\n",
        "        test_data[\"posts\"][username] = posts\n",
        "        test_post_count += len(posts)\n",
        "\n",
        "# Output the sizes of train and test sets\n",
        "print(f\"Number of users in train data: {len(train_data['profiles'])}\")\n",
        "print(f\"Number of posts in train data: {train_post_count}\")\n",
        "print(f\"Number of users in test data: {len(test_data['profiles'])}\")\n",
        "print(f\"Number of posts in test data: {test_post_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r066yba7u1sB",
        "outputId": "44752387-6983-4814-fd09-95ea3bd2d99f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of users in train data: 4332\n",
            "Number of posts in train data: 149943\n",
            "Number of users in test data: 1083\n",
            "Number of posts in test data: 37359\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Columns of train_data and test_data"
      ],
      "metadata": {
        "id": "UZsBbtQcL5Og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract all unique columns (keys) from the posts\n",
        "def extract_columns_from_data(data):\n",
        "    columns = set()  # Use a set to store unique column names\n",
        "    for posts in data[\"posts\"].values():\n",
        "        for post in posts:\n",
        "            columns.update(post.keys())  # Add all keys from the current post\n",
        "    return sorted(columns)  # Return sorted list of unique columns\n",
        "\n",
        "# Extract columns for train_data and test_data\n",
        "train_columns = extract_columns_from_data(train_data)\n",
        "test_columns = extract_columns_from_data(test_data)\n",
        "\n",
        "# Print the columns\n",
        "print(\"Columns in train_data posts:\")\n",
        "print(train_columns)\n",
        "\n",
        "print(\"\\nColumns in test_data posts:\")\n",
        "print(test_columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPRRXN9OLD-A",
        "outputId": "518c6bbc-9736-480b-d0a2-ca0ea40971ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in train_data posts:\n",
            "['caption', 'comments_count', 'id', 'like_count', 'media_type', 'media_url', 'timestamp']\n",
            "\n",
            "Columns in test_data posts:\n",
            "['caption', 'comments_count', 'id', 'like_count', 'media_type', 'media_url', 'timestamp']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Future Extraction"
      ],
      "metadata": {
        "id": "pcc09q94HDaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Function to extract features and targets\n",
        "def extract_features_and_targets(data):\n",
        "    features = []\n",
        "    targets = []\n",
        "\n",
        "    for username, posts in data[\"posts\"].items():\n",
        "        for post in posts:\n",
        "            caption = post.get(\"caption\", \"\") or \"\"\n",
        "            feature = [\n",
        "                len(caption),  # Caption length\n",
        "                caption.count(\"#\"),  # Number of hashtags\n",
        "                caption.count(\"@\"),  # Number of mentions\n",
        "                post.get(\"comments_count\", 0) or 0,  # Number of comments\n",
        "                1 if post.get(\"media_type\") == \"IMAGE\" else 0  # Media type\n",
        "            ]\n",
        "\n",
        "            # Add time-based features\n",
        "            timestamp = post.get(\"timestamp\")\n",
        "            if timestamp:\n",
        "                dt = datetime.fromisoformat(timestamp)\n",
        "                feature.append(dt.hour)  # Hour of posting\n",
        "                feature.append(dt.weekday())  # Day of the week\n",
        "                feature.append(1 if dt.weekday() >= 5 else 0)  # Weekend indicator\n",
        "\n",
        "            like_count = post.get(\"like_count\", None)\n",
        "            if like_count is not None:\n",
        "                features.append(feature)\n",
        "                targets.append(int(like_count))  # Ensure target is an integer\n",
        "\n",
        "    return np.array(features), np.array(targets)\n",
        "\n",
        "# Extract features and targets for train and test datasets\n",
        "train_features, train_targets = extract_features_and_targets(train_data)\n",
        "test_features, test_targets = extract_features_and_targets(test_data)\n",
        "\n",
        "# Handle outliers: Remove extreme targets based on IQR\n",
        "q1 = np.percentile(train_targets, 25)\n",
        "q3 = np.percentile(train_targets, 75)\n",
        "iqr = q3 - q1\n",
        "lower_bound = q1 - 1.5 * iqr\n",
        "upper_bound = q3 + 1.5 * iqr\n",
        "filtered_indices = (train_targets >= lower_bound) & (train_targets <= upper_bound)\n",
        "\n",
        "train_features = train_features[filtered_indices]\n",
        "train_targets = train_targets[filtered_indices]\n",
        "\n",
        "# Feature Scaling for Time-based Features\n",
        "scaler = MinMaxScaler()\n",
        "train_features[:, 5:7] = scaler.fit_transform(train_features[:, 5:7])  # Scale hour and weekday\n",
        "test_features[:, 5:7] = scaler.transform(test_features[:, 5:7])\n"
      ],
      "metadata": {
        "id": "HKDgJJ3ZG-gE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural network"
      ],
      "metadata": {
        "id": "jTUZSRJ6SpBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Train/Validation/Test Split + Scaling\n",
        "# -----------------------------\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    train_features,  # your original train_features\n",
        "    train_targets,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled   = scaler.transform(X_val)\n",
        "X_test_scaled  = scaler.transform(test_features)\n",
        "\n",
        "y_train_log = np.log1p(y_train)\n",
        "y_val_log   = np.log1p(y_val)\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Build the Model w/ Stronger Reg\n",
        "# -----------------------------\n",
        "def create_regression_model(input_dim):\n",
        "    l2_reg = regularizers.l2(1e-3)  # stronger L2\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=(input_dim,)),\n",
        "\n",
        "        layers.Dense(64, activation='relu', kernel_regularizer=l2_reg),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.4),  # increased dropout\n",
        "\n",
        "        layers.Dense(64, activation='relu', kernel_regularizer=l2_reg),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.4),\n",
        "\n",
        "        layers.Dense(1, activation='linear')\n",
        "    ])\n",
        "\n",
        "    # Use gradient clipping in the optimizer\n",
        "    optimizer = keras.optimizers.Adam(\n",
        "        learning_rate=1e-4,\n",
        "        clipnorm=1.0\n",
        "    )\n",
        "\n",
        "    model.compile(loss='mse', optimizer=optimizer, metrics=[])\n",
        "    return model\n",
        "\n",
        "nn_model = create_regression_model(X_train_scaled.shape[1])\n",
        "\n",
        "# -----------------------------\n",
        "# 3) EarlyStopping\n",
        "# -----------------------------\n",
        "early_stop = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=20,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 4) Train the Model\n",
        "# -----------------------------\n",
        "history = nn_model.fit(\n",
        "    X_train_scaled,\n",
        "    y_train_log,\n",
        "    validation_data=(X_val_scaled, y_val_log),\n",
        "    epochs=300,\n",
        "    batch_size=512,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 5) Evaluate on Validation Set\n",
        "# -----------------------------\n",
        "val_pred_log = nn_model.predict(X_val_scaled).ravel()\n",
        "\n",
        "# Clip predictions in log space to avoid huge exponents\n",
        "val_pred_log_clipped = np.clip(val_pred_log, 0.0, 15.0)\n",
        "val_pred = np.round(np.expm1(val_pred_log_clipped)).astype(int)\n",
        "\n",
        "val_log_mse = np.mean((np.log1p(y_val) - np.log1p(val_pred))**2)\n",
        "val_r2      = r2_score(y_val, val_pred)\n",
        "\n",
        "print(\"Validation Log MSE (NN):\", val_log_mse)\n",
        "print(\"Validation R² (NN):\", val_r2)\n",
        "\n",
        "# -----------------------------\n",
        "# 6) Evaluate on Test Set\n",
        "# -----------------------------\n",
        "y_test_log = np.log1p(test_targets)\n",
        "test_pred_log = nn_model.predict(X_test_scaled).ravel()\n",
        "\n",
        "# Again, clip to avoid overflow\n",
        "test_pred_log_clipped = np.clip(test_pred_log, 0.0, 15.0)\n",
        "test_pred = np.round(np.expm1(test_pred_log_clipped)).astype(int)\n",
        "\n",
        "# If test_pred is 0 or negative, log1p(<=0) might be invalid.\n",
        "# But typically it should be >= 0 if we've clipped to min=0.0 in log space.\n",
        "test_log_mse = np.mean((np.log1p(test_targets) - np.log1p(test_pred))**2)\n",
        "test_r2 = r2_score(test_targets, test_pred)\n",
        "\n",
        "print(\"Test Log MSE (NN):\", test_log_mse)\n",
        "print(\"Test R² (NN):\", test_r2)\n",
        "\n",
        "# Sample Predictions\n",
        "print(\"\\nSample Predictions (NN):\")\n",
        "for true, pred in zip(test_targets[:10], test_pred[:10]):\n",
        "    print(f\"Actual: {true}, Predicted: {pred}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5BFE7mWWDoB",
        "outputId": "d2e6a964-7695-4876-f035-9951b1321535"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 18.8212 - val_loss: 16.7569\n",
            "Epoch 2/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 16.9191 - val_loss: 15.3330\n",
            "Epoch 3/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 15.2765 - val_loss: 13.6793\n",
            "Epoch 4/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 13.7895 - val_loss: 11.9174\n",
            "Epoch 5/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 12.3059 - val_loss: 10.1964\n",
            "Epoch 6/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 10.6478 - val_loss: 8.3039\n",
            "Epoch 7/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 9.1059 - val_loss: 6.5531\n",
            "Epoch 8/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 7.6842 - val_loss: 4.9596\n",
            "Epoch 9/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 6.3483 - val_loss: 3.6977\n",
            "Epoch 10/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 5.3301 - val_loss: 2.7498\n",
            "Epoch 11/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.4903 - val_loss: 2.1162\n",
            "Epoch 12/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 4.0152 - val_loss: 1.8109\n",
            "Epoch 13/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 3.6830 - val_loss: 1.7034\n",
            "Epoch 14/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 3.4110 - val_loss: 1.6701\n",
            "Epoch 15/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3.2947 - val_loss: 1.6485\n",
            "Epoch 16/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 3.1611 - val_loss: 1.6277\n",
            "Epoch 17/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 3.0295 - val_loss: 1.6171\n",
            "Epoch 18/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 2.8805 - val_loss: 1.5832\n",
            "Epoch 19/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.7157 - val_loss: 1.5252\n",
            "Epoch 20/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.6385 - val_loss: 1.4739\n",
            "Epoch 21/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.5168 - val_loss: 1.4330\n",
            "Epoch 22/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.3933 - val_loss: 1.4028\n",
            "Epoch 23/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.3381 - val_loss: 1.3800\n",
            "Epoch 24/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.2214 - val_loss: 1.3572\n",
            "Epoch 25/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.1773 - val_loss: 1.3420\n",
            "Epoch 26/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.1263 - val_loss: 1.3238\n",
            "Epoch 27/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 2.0951 - val_loss: 1.3100\n",
            "Epoch 28/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 2.0018 - val_loss: 1.3001\n",
            "Epoch 29/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.9960 - val_loss: 1.2926\n",
            "Epoch 30/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.9424 - val_loss: 1.2880\n",
            "Epoch 31/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.9259 - val_loss: 1.2804\n",
            "Epoch 32/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8620 - val_loss: 1.2723\n",
            "Epoch 33/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8353 - val_loss: 1.2709\n",
            "Epoch 34/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8331 - val_loss: 1.2612\n",
            "Epoch 35/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8217 - val_loss: 1.2591\n",
            "Epoch 36/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.7766 - val_loss: 1.2541\n",
            "Epoch 37/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7647 - val_loss: 1.2499\n",
            "Epoch 38/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.7520 - val_loss: 1.2502\n",
            "Epoch 39/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 1.7401 - val_loss: 1.2469\n",
            "Epoch 40/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 1.7036 - val_loss: 1.2455\n",
            "Epoch 41/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.6885 - val_loss: 1.2410\n",
            "Epoch 42/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6715 - val_loss: 1.2382\n",
            "Epoch 43/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6523 - val_loss: 1.2359\n",
            "Epoch 44/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6597 - val_loss: 1.2351\n",
            "Epoch 45/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6380 - val_loss: 1.2340\n",
            "Epoch 46/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6369 - val_loss: 1.2317\n",
            "Epoch 47/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6329 - val_loss: 1.2293\n",
            "Epoch 48/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.5986 - val_loss: 1.2265\n",
            "Epoch 49/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.5849 - val_loss: 1.2266\n",
            "Epoch 50/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 1.5927 - val_loss: 1.2251\n",
            "Epoch 51/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 1.5717 - val_loss: 1.2278\n",
            "Epoch 52/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.5602 - val_loss: 1.2193\n",
            "Epoch 53/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.5523 - val_loss: 1.2210\n",
            "Epoch 54/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.5435 - val_loss: 1.2136\n",
            "Epoch 55/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.5375 - val_loss: 1.2113\n",
            "Epoch 56/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.5185 - val_loss: 1.2146\n",
            "Epoch 57/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.5265 - val_loss: 1.2097\n",
            "Epoch 58/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.4968 - val_loss: 1.2082\n",
            "Epoch 59/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.5127 - val_loss: 1.2090\n",
            "Epoch 60/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.5015 - val_loss: 1.2081\n",
            "Epoch 61/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.5020 - val_loss: 1.2065\n",
            "Epoch 62/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 1.4873 - val_loss: 1.2072\n",
            "Epoch 63/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.4810 - val_loss: 1.2057\n",
            "Epoch 64/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.4664 - val_loss: 1.2023\n",
            "Epoch 65/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.4577 - val_loss: 1.2045\n",
            "Epoch 66/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.4541 - val_loss: 1.2039\n",
            "Epoch 67/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.4529 - val_loss: 1.2003\n",
            "Epoch 68/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.4442 - val_loss: 1.2005\n",
            "Epoch 69/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.4297 - val_loss: 1.1984\n",
            "Epoch 70/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.4369 - val_loss: 1.1967\n",
            "Epoch 71/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.4415 - val_loss: 1.1989\n",
            "Epoch 72/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1.4279 - val_loss: 1.1975\n",
            "Epoch 73/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 1.4269 - val_loss: 1.1940\n",
            "Epoch 74/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.4287 - val_loss: 1.1965\n",
            "Epoch 75/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.4126 - val_loss: 1.1938\n",
            "Epoch 76/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.4006 - val_loss: 1.1916\n",
            "Epoch 77/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.4043 - val_loss: 1.1874\n",
            "Epoch 78/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.4032 - val_loss: 1.1886\n",
            "Epoch 79/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.3979 - val_loss: 1.1858\n",
            "Epoch 80/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.3871 - val_loss: 1.1950\n",
            "Epoch 81/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.3721 - val_loss: 1.1830\n",
            "Epoch 82/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 1.3830 - val_loss: 1.1859\n",
            "Epoch 83/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 1.3790 - val_loss: 1.1883\n",
            "Epoch 84/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.3673 - val_loss: 1.1816\n",
            "Epoch 85/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.3621 - val_loss: 1.1793\n",
            "Epoch 86/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.3613 - val_loss: 1.1791\n",
            "Epoch 87/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.3732 - val_loss: 1.1752\n",
            "Epoch 88/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.3533 - val_loss: 1.1805\n",
            "Epoch 89/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.3520 - val_loss: 1.1766\n",
            "Epoch 90/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.3466 - val_loss: 1.1741\n",
            "Epoch 91/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.3493 - val_loss: 1.1725\n",
            "Epoch 92/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.3426 - val_loss: 1.1747\n",
            "Epoch 93/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.3547 - val_loss: 1.1728\n",
            "Epoch 94/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 1.3500 - val_loss: 1.1699\n",
            "Epoch 95/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.3323 - val_loss: 1.1730\n",
            "Epoch 96/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.3340 - val_loss: 1.1699\n",
            "Epoch 97/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.3392 - val_loss: 1.1681\n",
            "Epoch 98/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.3252 - val_loss: 1.1666\n",
            "Epoch 99/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.3148 - val_loss: 1.1682\n",
            "Epoch 100/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 1.3234 - val_loss: 1.1699\n",
            "Epoch 101/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 1.3242 - val_loss: 1.1653\n",
            "Epoch 102/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 1.3196 - val_loss: 1.1655\n",
            "Epoch 103/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 1.3100 - val_loss: 1.1649\n",
            "Epoch 104/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 1.3147 - val_loss: 1.1619\n",
            "Epoch 105/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.3201 - val_loss: 1.1627\n",
            "Epoch 106/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.3115 - val_loss: 1.1622\n",
            "Epoch 107/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.3022 - val_loss: 1.1601\n",
            "Epoch 108/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.3158 - val_loss: 1.1606\n",
            "Epoch 109/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.3131 - val_loss: 1.1586\n",
            "Epoch 110/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.3018 - val_loss: 1.1567\n",
            "Epoch 111/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2890 - val_loss: 1.1593\n",
            "Epoch 112/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.3007 - val_loss: 1.1588\n",
            "Epoch 113/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.3018 - val_loss: 1.1574\n",
            "Epoch 114/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 1.2823 - val_loss: 1.1544\n",
            "Epoch 115/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.2925 - val_loss: 1.1540\n",
            "Epoch 116/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2898 - val_loss: 1.1542\n",
            "Epoch 117/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2916 - val_loss: 1.1543\n",
            "Epoch 118/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2915 - val_loss: 1.1533\n",
            "Epoch 119/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2917 - val_loss: 1.1516\n",
            "Epoch 120/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2784 - val_loss: 1.1538\n",
            "Epoch 121/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2979 - val_loss: 1.1534\n",
            "Epoch 122/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2795 - val_loss: 1.1510\n",
            "Epoch 123/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2687 - val_loss: 1.1513\n",
            "Epoch 124/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.2837 - val_loss: 1.1498\n",
            "Epoch 125/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 1.2748 - val_loss: 1.1502\n",
            "Epoch 126/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.2774 - val_loss: 1.1507\n",
            "Epoch 127/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2620 - val_loss: 1.1471\n",
            "Epoch 128/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2685 - val_loss: 1.1499\n",
            "Epoch 129/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2699 - val_loss: 1.1475\n",
            "Epoch 130/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2635 - val_loss: 1.1506\n",
            "Epoch 131/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2646 - val_loss: 1.1492\n",
            "Epoch 132/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2603 - val_loss: 1.1489\n",
            "Epoch 133/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2584 - val_loss: 1.1459\n",
            "Epoch 134/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2659 - val_loss: 1.1480\n",
            "Epoch 135/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.2506 - val_loss: 1.1458\n",
            "Epoch 136/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 1.2542 - val_loss: 1.1430\n",
            "Epoch 137/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.2584 - val_loss: 1.1441\n",
            "Epoch 138/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2390 - val_loss: 1.1433\n",
            "Epoch 139/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2464 - val_loss: 1.1443\n",
            "Epoch 140/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2411 - val_loss: 1.1433\n",
            "Epoch 141/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.2488 - val_loss: 1.1439\n",
            "Epoch 142/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2545 - val_loss: 1.1424\n",
            "Epoch 143/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2525 - val_loss: 1.1409\n",
            "Epoch 144/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2308 - val_loss: 1.1413\n",
            "Epoch 145/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2404 - val_loss: 1.1427\n",
            "Epoch 146/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 1.2456 - val_loss: 1.1412\n",
            "Epoch 147/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 1.2416 - val_loss: 1.1397\n",
            "Epoch 148/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.2429 - val_loss: 1.1420\n",
            "Epoch 149/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2305 - val_loss: 1.1406\n",
            "Epoch 150/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2366 - val_loss: 1.1406\n",
            "Epoch 151/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2404 - val_loss: 1.1391\n",
            "Epoch 152/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2443 - val_loss: 1.1382\n",
            "Epoch 153/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2400 - val_loss: 1.1411\n",
            "Epoch 154/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.2380 - val_loss: 1.1404\n",
            "Epoch 155/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2328 - val_loss: 1.1422\n",
            "Epoch 156/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 1.2259 - val_loss: 1.1409\n",
            "Epoch 157/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 1.2321 - val_loss: 1.1398\n",
            "Epoch 158/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.2348 - val_loss: 1.1393\n",
            "Epoch 159/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.2222 - val_loss: 1.1409\n",
            "Epoch 160/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2318 - val_loss: 1.1401\n",
            "Epoch 161/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2280 - val_loss: 1.1384\n",
            "Epoch 162/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2288 - val_loss: 1.1420\n",
            "Epoch 163/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.2225 - val_loss: 1.1418\n",
            "Epoch 164/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2215 - val_loss: 1.1401\n",
            "Epoch 165/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2142 - val_loss: 1.1342\n",
            "Epoch 166/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 1.2208 - val_loss: 1.1392\n",
            "Epoch 167/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 1.2215 - val_loss: 1.1362\n",
            "Epoch 168/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1.2251 - val_loss: 1.1403\n",
            "Epoch 169/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.2185 - val_loss: 1.1348\n",
            "Epoch 170/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2104 - val_loss: 1.1355\n",
            "Epoch 171/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2127 - val_loss: 1.1365\n",
            "Epoch 172/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.2203 - val_loss: 1.1370\n",
            "Epoch 173/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2207 - val_loss: 1.1337\n",
            "Epoch 174/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2115 - val_loss: 1.1396\n",
            "Epoch 175/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.2111 - val_loss: 1.1368\n",
            "Epoch 176/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 1.2143 - val_loss: 1.1403\n",
            "Epoch 177/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 1.2048 - val_loss: 1.1354\n",
            "Epoch 178/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2138 - val_loss: 1.1367\n",
            "Epoch 179/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2151 - val_loss: 1.1354\n",
            "Epoch 180/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2065 - val_loss: 1.1346\n",
            "Epoch 181/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2029 - val_loss: 1.1346\n",
            "Epoch 182/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2179 - val_loss: 1.1347\n",
            "Epoch 183/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2051 - val_loss: 1.1330\n",
            "Epoch 184/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2077 - val_loss: 1.1354\n",
            "Epoch 185/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2001 - val_loss: 1.1353\n",
            "Epoch 186/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 1.2019 - val_loss: 1.1360\n",
            "Epoch 187/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 1.1964 - val_loss: 1.1354\n",
            "Epoch 188/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.2008 - val_loss: 1.1351\n",
            "Epoch 189/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2051 - val_loss: 1.1308\n",
            "Epoch 190/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1981 - val_loss: 1.1330\n",
            "Epoch 191/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2053 - val_loss: 1.1344\n",
            "Epoch 192/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1909 - val_loss: 1.1337\n",
            "Epoch 193/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2122 - val_loss: 1.1354\n",
            "Epoch 194/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1900 - val_loss: 1.1307\n",
            "Epoch 195/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1941 - val_loss: 1.1278\n",
            "Epoch 196/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1999 - val_loss: 1.1334\n",
            "Epoch 197/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1914 - val_loss: 1.1313\n",
            "Epoch 198/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 1.1977 - val_loss: 1.1380\n",
            "Epoch 199/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 1.1942 - val_loss: 1.1316\n",
            "Epoch 200/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.2104 - val_loss: 1.1299\n",
            "Epoch 201/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1924 - val_loss: 1.1293\n",
            "Epoch 202/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1870 - val_loss: 1.1307\n",
            "Epoch 203/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1941 - val_loss: 1.1312\n",
            "Epoch 204/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1906 - val_loss: 1.1326\n",
            "Epoch 205/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1944 - val_loss: 1.1293\n",
            "Epoch 206/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1884 - val_loss: 1.1319\n",
            "Epoch 207/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1845 - val_loss: 1.1297\n",
            "Epoch 208/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1847 - val_loss: 1.1292\n",
            "Epoch 209/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.1821 - val_loss: 1.1328\n",
            "Epoch 210/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 1.1924 - val_loss: 1.1289\n",
            "Epoch 211/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1775 - val_loss: 1.1295\n",
            "Epoch 212/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1837 - val_loss: 1.1293\n",
            "Epoch 213/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1814 - val_loss: 1.1248\n",
            "Epoch 214/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1798 - val_loss: 1.1273\n",
            "Epoch 215/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1751 - val_loss: 1.1303\n",
            "Epoch 216/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1909 - val_loss: 1.1278\n",
            "Epoch 217/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1787 - val_loss: 1.1277\n",
            "Epoch 218/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1799 - val_loss: 1.1311\n",
            "Epoch 219/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1.1813 - val_loss: 1.1289\n",
            "Epoch 220/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 1.1770 - val_loss: 1.1309\n",
            "Epoch 221/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1760 - val_loss: 1.1274\n",
            "Epoch 222/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1729 - val_loss: 1.1297\n",
            "Epoch 223/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1752 - val_loss: 1.1283\n",
            "Epoch 224/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1735 - val_loss: 1.1240\n",
            "Epoch 225/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1767 - val_loss: 1.1258\n",
            "Epoch 226/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1725 - val_loss: 1.1271\n",
            "Epoch 227/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1855 - val_loss: 1.1230\n",
            "Epoch 228/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1793 - val_loss: 1.1262\n",
            "Epoch 229/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1785 - val_loss: 1.1286\n",
            "Epoch 230/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 1.1755 - val_loss: 1.1265\n",
            "Epoch 231/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1.1819 - val_loss: 1.1235\n",
            "Epoch 232/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1760 - val_loss: 1.1281\n",
            "Epoch 233/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1757 - val_loss: 1.1278\n",
            "Epoch 234/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1733 - val_loss: 1.1217\n",
            "Epoch 235/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1687 - val_loss: 1.1257\n",
            "Epoch 236/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1803 - val_loss: 1.1214\n",
            "Epoch 237/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1668 - val_loss: 1.1241\n",
            "Epoch 238/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1737 - val_loss: 1.1251\n",
            "Epoch 239/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1716 - val_loss: 1.1211\n",
            "Epoch 240/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1775 - val_loss: 1.1263\n",
            "Epoch 241/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 1.1674 - val_loss: 1.1234\n",
            "Epoch 242/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 1.1758 - val_loss: 1.1210\n",
            "Epoch 243/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.1666 - val_loss: 1.1190\n",
            "Epoch 244/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1651 - val_loss: 1.1239\n",
            "Epoch 245/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1725 - val_loss: 1.1266\n",
            "Epoch 246/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1630 - val_loss: 1.1210\n",
            "Epoch 247/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1750 - val_loss: 1.1216\n",
            "Epoch 248/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1610 - val_loss: 1.1218\n",
            "Epoch 249/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1643 - val_loss: 1.1194\n",
            "Epoch 250/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1641 - val_loss: 1.1204\n",
            "Epoch 251/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1567 - val_loss: 1.1166\n",
            "Epoch 252/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.1598 - val_loss: 1.1215\n",
            "Epoch 253/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 1.1746 - val_loss: 1.1199\n",
            "Epoch 254/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1573 - val_loss: 1.1244\n",
            "Epoch 255/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1664 - val_loss: 1.1204\n",
            "Epoch 256/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1581 - val_loss: 1.1175\n",
            "Epoch 257/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1682 - val_loss: 1.1178\n",
            "Epoch 258/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1598 - val_loss: 1.1184\n",
            "Epoch 259/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1639 - val_loss: 1.1187\n",
            "Epoch 260/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1617 - val_loss: 1.1184\n",
            "Epoch 261/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1611 - val_loss: 1.1196\n",
            "Epoch 262/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 1.1681 - val_loss: 1.1218\n",
            "Epoch 263/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 1.1598 - val_loss: 1.1126\n",
            "Epoch 264/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.1610 - val_loss: 1.1154\n",
            "Epoch 265/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1599 - val_loss: 1.1121\n",
            "Epoch 266/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1584 - val_loss: 1.1190\n",
            "Epoch 267/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1660 - val_loss: 1.1197\n",
            "Epoch 268/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1555 - val_loss: 1.1186\n",
            "Epoch 269/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1540 - val_loss: 1.1161\n",
            "Epoch 270/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1571 - val_loss: 1.1139\n",
            "Epoch 271/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 1.1622 - val_loss: 1.1153\n",
            "Epoch 272/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 1.1586 - val_loss: 1.1166\n",
            "Epoch 273/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.1573 - val_loss: 1.1200\n",
            "Epoch 274/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1530 - val_loss: 1.1136\n",
            "Epoch 275/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1555 - val_loss: 1.1154\n",
            "Epoch 276/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1585 - val_loss: 1.1122\n",
            "Epoch 277/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1546 - val_loss: 1.1123\n",
            "Epoch 278/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1598 - val_loss: 1.1124\n",
            "Epoch 279/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1500 - val_loss: 1.1146\n",
            "Epoch 280/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1501 - val_loss: 1.1211\n",
            "Epoch 281/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1611 - val_loss: 1.1163\n",
            "Epoch 282/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 1.1578 - val_loss: 1.1109\n",
            "Epoch 283/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 1.1462 - val_loss: 1.1152\n",
            "Epoch 284/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1571 - val_loss: 1.1210\n",
            "Epoch 285/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1516 - val_loss: 1.1151\n",
            "Epoch 286/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1551 - val_loss: 1.1205\n",
            "Epoch 287/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1531 - val_loss: 1.1118\n",
            "Epoch 288/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1540 - val_loss: 1.1144\n",
            "Epoch 289/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1534 - val_loss: 1.1130\n",
            "Epoch 290/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1515 - val_loss: 1.1158\n",
            "Epoch 291/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1477 - val_loss: 1.1153\n",
            "Epoch 292/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 1.1428 - val_loss: 1.1165\n",
            "Epoch 293/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 1.1458 - val_loss: 1.1144\n",
            "Epoch 294/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.1399 - val_loss: 1.1113\n",
            "Epoch 295/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1412 - val_loss: 1.1135\n",
            "Epoch 296/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1513 - val_loss: 1.1141\n",
            "Epoch 297/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1424 - val_loss: 1.1137\n",
            "Epoch 298/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1507 - val_loss: 1.1131\n",
            "Epoch 299/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1564 - val_loss: 1.1142\n",
            "Epoch 300/300\n",
            "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.1446 - val_loss: 1.1155\n",
            "\u001b[1m752/752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Validation Log MSE (NN): 1.099877690017771\n",
            "Validation R² (NN): 0.07106787611229204\n",
            "\u001b[1m1142/1142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
            "Test Log MSE (NN): 3.701033419929791\n",
            "Test R² (NN): -0.28522922226832437\n",
            "\n",
            "Sample Predictions (NN):\n",
            "Actual: 20, Predicted: 49\n",
            "Actual: 56, Predicted: 57\n",
            "Actual: 21, Predicted: 24\n",
            "Actual: 18, Predicted: 31\n",
            "Actual: 41, Predicted: 235\n",
            "Actual: 141, Predicted: 245\n",
            "Actual: 285, Predicted: 26\n",
            "Actual: 664, Predicted: 217\n",
            "Actual: 1376, Predicted: 220\n",
            "Actual: 54, Predicted: 243\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}